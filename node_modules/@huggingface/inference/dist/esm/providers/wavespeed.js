import { delay } from "../utils/delay.js";
import { omit } from "../utils/omit.js";
import { base64FromBytes } from "../utils/base64FromBytes.js";
import { TaskProviderHelper } from "./providerHelper.js";
import { InferenceClientInputError, InferenceClientProviderApiError, InferenceClientProviderOutputError, } from "../errors.js";
const WAVESPEEDAI_API_BASE_URL = "https://api.wavespeed.ai";
class WavespeedAITask extends TaskProviderHelper {
    constructor(url) {
        super("wavespeed", url || WAVESPEEDAI_API_BASE_URL);
    }
    makeRoute(params) {
        return `/api/v3/${params.model}`;
    }
    preparePayload(params) {
        const payload = {
            ...omit(params.args, ["inputs", "parameters"]),
            ...params.args.parameters,
            prompt: params.args.inputs,
        };
        // Add LoRA support if adapter is specified in the mapping
        if (params.mapping?.adapter === "lora") {
            payload.loras = [
                {
                    path: params.mapping.hfModelId,
                    scale: 1, // Default scale value
                },
            ];
        }
        return payload;
    }
    async getResponse(response, url, headers) {
        if (!headers) {
            throw new InferenceClientInputError("Headers are required for WaveSpeed AI API calls");
        }
        const resultUrl = response.data.urls.get;
        // Poll for results until completion
        while (true) {
            const resultResponse = await fetch(resultUrl, { headers });
            if (!resultResponse.ok) {
                throw new InferenceClientProviderApiError("Failed to fetch response status from WaveSpeed AI API", { url: resultUrl, method: "GET" }, {
                    requestId: resultResponse.headers.get("x-request-id") ?? "",
                    status: resultResponse.status,
                    body: await resultResponse.text(),
                });
            }
            const result = await resultResponse.json();
            const taskResult = result.data;
            switch (taskResult.status) {
                case "completed": {
                    // Get the media data from the first output URL
                    if (!taskResult.outputs?.[0]) {
                        throw new InferenceClientProviderOutputError("Received malformed response from WaveSpeed AI API: No output URL in completed response");
                    }
                    const mediaResponse = await fetch(taskResult.outputs[0]);
                    if (!mediaResponse.ok) {
                        throw new InferenceClientProviderApiError("Failed to fetch generation output from WaveSpeed AI API", { url: taskResult.outputs[0], method: "GET" }, {
                            requestId: mediaResponse.headers.get("x-request-id") ?? "",
                            status: mediaResponse.status,
                            body: await mediaResponse.text(),
                        });
                    }
                    return await mediaResponse.blob();
                }
                case "failed": {
                    throw new InferenceClientProviderOutputError(taskResult.error || "Task failed");
                }
                default: {
                    // Wait before polling again
                    await delay(500);
                    continue;
                }
            }
        }
    }
}
export class WavespeedAITextToImageTask extends WavespeedAITask {
    constructor() {
        super(WAVESPEEDAI_API_BASE_URL);
    }
}
export class WavespeedAITextToVideoTask extends WavespeedAITask {
    constructor() {
        super(WAVESPEEDAI_API_BASE_URL);
    }
}
export class WavespeedAIImageToImageTask extends WavespeedAITask {
    constructor() {
        super(WAVESPEEDAI_API_BASE_URL);
    }
    async preparePayloadAsync(args) {
        return {
            ...args,
            inputs: args.parameters?.prompt,
            image: base64FromBytes(new Uint8Array(args.inputs instanceof ArrayBuffer ? args.inputs : await args.inputs.arrayBuffer())),
        };
    }
}
